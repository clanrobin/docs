Chapter 10 - Stanford Stats Course
---------------------------------
Back to the homepage https://clanrobin.github.io

Principal Components
====================
We will use the `USArrests` data (which is in R)
```{r}
dimnames(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2, var)
```

We see that `Assault` has a much larger variance than the other variables. It would dominate the principal components, so we choose to standardize the variables when we perform PCA

```{r}
pca.out=prcomp(USArrests, scale=TRUE)
pca.out
#help(prcomp)
names(pca.out)
biplot(pca.out, scale=0)
biplot(pca.out, scale=0, cex=0.6)
```

K-Means Clustering
==================
K-means works in any dimension, but is most fun to demonstrate in two, because we can plot pictures.
Lets make some data with clusters. We do this by shifting the means of the points around.
```{r}
set.seed(101)
x=matrix(rnorm(100*2),100,2)
xmean=matrix(rnorm(8,sd=4),4,2)
which=sample(1:4,100,replace=TRUE)
x=x+xmean[which,]
plot(x,col=which,pch=19)
```

We know the "true" cluster IDs, but we wont tell that to the `kmeans` algorithm.

```{r}
km.out=kmeans(x,4,nstart=15)
km.out
plot(x,col=km.out$cluster,cex=2,pch=1,lwd=2)
points(x,col=which,pch=19)
points(x,col=c(3,2,4,1)[which],pch=19)
```

Ideally I'd get the points and the circles to match colours, but it seems R behaves a bit strangely here. What you need to look for are the two points which are misclassified. There is one from group 2 (left to right) that is mixed into group 1 and one from group 3 which is mixed into group 2.


Hierarchical Clustering
=======================
We will use these same data and use hierarchical clustering

```{r}
hc.complete=hclust(dist(x),method="complete")
plot(hc.complete)
hc.single=hclust(dist(x),method="single")
plot(hc.single)
hc.average=hclust(dist(x),method="average")
plot(hc.average)

```

Lets compare this with the actual clusters in the data. We will use the function `cutree` to cut the tree at level 4.
This will produce a vector of numbers from 1 to 4, saying which branch each observation is on. You will sometimes see pretty plots where the leaves of the dendrogram are colored. I searched a bit on the web for how to do this, and its a little too complicated for this demonstration.

We can use `table` to see how well they match:
```{r}
hc.cut=cutree(hc.complete,4)
table(hc.cut,which)
table(hc.cut,km.out$cluster)
```
or we can use our group membership as labels for the leaves of the dendrogram:
```{r}
plot(hc.complete,labels=which)
```

10.R.1
========
1 point possible (graded)

Suppose we want to fit a linear regression, but the number of variables is much larger than the number of observations. In some cases, we may improve the fit by reducing the dimension of the features before.

In this problem, we use a data set with n = 300 and p = 200, so we have more observations than variables, but not by much. Load the data x, y, x.test, and y.test from 10.R.RData.

First, concatenate x and x.test using the rbind functions and perform a principal components analysis on the concatenated data frame (use the "scale=TRUE" option). To within 10% relative error, what proportion of the variance is explained by the first five principal components?
 
Load the data
-------------

```{r}
data1 = load("/Users/bconn/Documents/ClanRobin/Stanford Stats Course/Rscripts/10.R.RData")
#fix(data1) # x, y x.test, y.test
?rbind
xx = rbind(x,x.test)
dim(xx)
pca.out=prcomp(xx, scale=TRUE)
#pca.out
help(prcomp)
names(pca.out)
#biplot(pca.out, scale=0)
biplot(pca.out, scale=0, cex=0.6)
```
Need to code the PVE equation

```{r}
vars = prcomp(xx,scale=TRUE)$sdev^2
sum(vars)
sum(vars[1:5])/200.

```

 10.R.2
========= 
0/1 point (graded)

The previous answer suggests that a relatively small number of "latent variables" account for a substantial fraction of the features' variability. We might believe that these latent variables are more important than linear combinations of the features that have low variance.

We can try forgetting about the raw features and using the first five principal components (computed on rbind(x,x.test)) instead as low-dimensional derived features. What is the mean-squared test error if we regress y on the first five principal components, and use the resulting model to predict y.test?

**I haven't got this working yet**

```{r}

pc5 = pca.out$x[1:300,1:5]
pc5.fit <- lm(y~pc5)
summary(pc5.fit)

xxx = pca.out$x[301:600,1:5]

pcdat = data.frame(x = xxx)
yuck = predict(pc5.fit,pcdat)

x11 = mean((yuck-y.test[1:300])^2)
x11
xxy = pca.out$x[601:900,1:5]

pcdat = data.frame(x = xxy)
yuck = predict(pc5.fit,pcdat)

x12 =mean((yuck-y.test[301:600])^2)
x12
xxz = pca.out$x[901:1200,1:5]

pcdat = data.frame(x = xxz)
yuck = predict(pc5.fit,pcdat)

x13= mean((yuck-y.test[1:300])^2)
x13

bla=predict(pc5.fit)

summary(yuck)



#myy = sm$coefficients[1] + sm$coefficients[2]*x.test+ sm$coefficients[3]*x.test+ sm$coefficients[4]*x.test+ sm$coefficients[5]*x.test+ sm$coefficients[6]*x.test 
#dim(myy)

#sm<-summary(fit.pc5)
#1-(mean(sm$residuals^2)-1)
#dim(y)

```
