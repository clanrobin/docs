Chapter 9 - Stanford Stats Course
---------------------------------
Back to the homepage https://clanrobin.github.io

SVM
========================================================
To demonstrate the SVM, it is easiest to work in low dimensions, so we can see the data.

Linear SVM classifier
---------------------
Lets generate some data in two dimensions, and make them a little separated.
```{r}
set.seed(10111)
x=matrix(rnorm(40),20,2)
y=rep(c(-1,1),c(10,10))
x[y==1,]=x[y==1,]+1
plot(x,col=y+3,pch=19)
```

Now we will load the package `e1071` which contains the `svm` function we will use. We then compute the fit. Notice that we have to specify a `cost` parameter, which is a tuning parameter. 
```{r}
library(e1071)
dat=data.frame(x,y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)
print(svmfit)
plot(svmfit,dat)
```

As mentioned in the the chapter, the plot function is somewhat crude, and plots X2 on the horizontal axis (unlike what R would do automatically for a matrix). Lets see how we might make our own plot.

The first thing we will do is make a grid of values for X1 and X2. We will write a function to do that,
in case we want to reuse it. It uses the handy function `expand.grid`, and produces the coordinates of `n*n` points on a lattice covering the domain of `x`. Having made the lattice, we make a prediction at each point on the lattice. We then plot the lattice, color-coded according to the classification. Now we can see the decision boundary.

The support points (points on the margin, or on the wrong side of the margin) are indexed in the `$index` component of the fit.

```{r}
make.grid=function(x,n=75){
  grange=apply(x,2,range)
  x1=seq(from=grange[1,1],to=grange[2,1],length=n)
  x2=seq(from=grange[1,2],to=grange[2,2],length=n)
  expand.grid(X1=x1,X2=x2)
  }
xgrid=make.grid(x)
xgrid[1:10,]
ygrid=predict(svmfit,xgrid)
plot(xgrid,col=c("red","blue")[as.numeric(ygrid)],pch=20,cex=.2)
points(x,col=y+3,pch=19)
points(x[svmfit$index,],pch=5,cex=2)
```

The `svm` function is not too friendly, in that we have to do some work to get back the linear coefficients, as described in the text. Probably the reason is that this only makes sense for linear kernels, and the function is more general. Here we will use a formula to extract the coefficients; for those interested in where this comes from, have a look in chapter 12 of ESL ("Elements of Statistical Learning").

We extract the linear coefficients, and then using simple algebra, we include the decision boundary and the two margins.

```{r}
beta=drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0=svmfit$rho
plot(xgrid,col=c("red","blue")[as.numeric(ygrid)],pch=20,cex=.2)
points(x,col=y+3,pch=19)
points(x[svmfit$index,],pch=5,cex=2)
abline(beta0/beta[2],-beta[1]/beta[2])
abline((beta0-1)/beta[2],-beta[1]/beta[2],lty=2)
abline((beta0+1)/beta[2],-beta[1]/beta[2],lty=2)
```

Just like for the other models in this book, the tuning parameter `C` has to be selected.
Different values will give different solutions. Rerun the code above, but using `C=1`, and see what we mean. One can use cross-validation to do this.


Nonlinear SVM
--------------
Instead, we will run the SVM on some data where a non-linear boundary is called for. We will use the mixture data from ESL

```{r}
load(url("http://www.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda"))
names(ESL.mixture)
rm(x,y)
attach(ESL.mixture)
```

These data are also two dimensional. Lets plot them and fit a nonlinear SVM, using a radial kernel.
```{r}
plot(x,col=y+1)
dat=data.frame(y=factor(y),x)
fit=svm(factor(y)~.,data=dat,scale=FALSE,kernel="radial",cost=5)
```

Now we are going to create a grid, as before, and make predictions on the grid.
These data have the grid points for each variable included on the data frame.
```{r}
xgrid=expand.grid(X1=px1,X2=px2)
ygrid=predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=.2)
points(x,col=y+1,pch=19)
```

We can go further, and have the predict function produce the actual function estimates at each of our grid points. We can include the actual decision boundary on the plot by making use of the contour function. On the dataframe is also `prob`, which is the true probability of class 1 for these data, at the gridpoints. If we plot its 0.5 contour, that will give us the _Bayes Decision Boundary_, which is the best one could ever do.
```{r}
func=predict(fit,xgrid,decision.values=TRUE)
func=attributes(func)$decision
xgrid=expand.grid(X1=px1,X2=px2)
ygrid=predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=.2)
points(x,col=y+1,pch=19)

contour(px1,px2,matrix(func,69,99),level=0,add=TRUE)
contour(px1,px2,matrix(prob,69,99),level=.5,add=TRUE,col="blue",lwd=2)
```

We see in this case that the radial kernel has done an excellent job.



9.R.1 lab
---------

In this problem, you will use simulation to evaluate (by Monte Carlo) the expected misclassification error rate given a particular generating model.  Let be equally divided between classes 0 and 1, and let

be normally distributed.

 Given 
 $$ y_i=0,x_i \sim N_{10}(0, I_{10})$$ 
 Given 
 $$ y_i=1,x_i \sim N_{10}(1, I_{10})$$ 
 with 
 $$\mu = (1,1,1,1,1,0,0,0,0,0)$$


Now, we would like to know the expected test error rate if we fit an SVM to a sample of 50 random training points from class 1 and 50 more from class 0.  We can calculate this to high precision by 1) generating a random training sample to train on, 2) evaluating the number of mistakes we make on a large test set, and then 3) repeating (1-2) many times and averaging the error rate for each trial.

Aside: in real life don't know the generating distribution, so we have to use resampling methods instead of the procedure described above.

For all of the following, please enter your error rate as a number between zero and 1 (e.g., 0.21 instead of 21 if the error rate is 21%).


```{r}
rm(x,y)
set.seed(239823645)
x=matrix(rnorm(200),100,2)
y=rep(c(1,0),c(50,50))
x[y==1,]=x[y==1,]+1
plot(x,col=y+3,pch=19)
y
```
calculate the svm fit with a cost of 5
```{r}
dat=data.frame(y=factor(y),x)
fit=svm(factor(y)~.,data=dat,scale=FALSE,kernel="radial",cost=5)
```

make the grid to plot the smooth results, with a little tweak, plot the decision boundary
```{r}
mymake.grid=function(x,n=75){
  grange=apply(x,2,range)
  x1=seq(from=grange[1,1],to=grange[2,1],length=n)
  x2=seq(from=grange[1,2],to=grange[2,2],length=n)
  return(list("newxgrid"=expand.grid(X1=x1,X2=x2), "newx1"=x1, "newx2"=x2))
  }

output = mymake.grid(x,n=100)

#output$newxgrid


newygrid=predict(fit,output$newxgrid)
plot(output$newxgrid,col=as.numeric(newygrid)+2,pch=20,cex=.2)
points(x,col=y+3,pch=19)

func=predict(fit,output$newxgrid,decision.values=TRUE)
func=attributes(func)$decision
contour(output$newx1,output$newx2,matrix(func,100,100),level=0,add=TRUE)
```

now we need to calculate the error rate.
that is, for a given point, how many are on the wrong side of the boundary?
evaluate the fit at the data points, take the average with the input data, find out how many have a value of 0.5

```{r}
out_y = as.numeric(predict(fit,x)) - 1
results = (out_y + y)/2.
error = length(subset(results, results=="0.5"))/length(output$newx1)
error
```

So at this point, we need to loop this process and take the average of the errors.

```{r}

set.seed(235)
counts= 1000
test.err = double(counts)
for (mtry in 1:counts){
  x=matrix(rnorm(1000),ncol=10)
  y=rep(c(1,0),c(50,50))
  x[y==1,1:5]=x[y==1,1:5]+1
  
  #dat=data.frame(y=factor(y),x)  #my version
  #fit=svm(factor(y)~.,data=dat)  #my version
  
  #online version - stackoverflow
  dat = data.frame(x = x, y = as.factor(y))
  svm.fit = svm(y ~ ., data = dat, kernel = "linear", cost = 1) #linear #0.15791
  #svm.fit = svm(y ~ ., data = dat) #default radial ##this version answer is meant to be 0.16350

  ##the above does seem to work-ish, I get 0.16922 in both cases
  
  
  ##my estimate of the percentage
  #out_y = as.numeric(predict(fit,x)) - 1
  #results = (out_y + y)/2.
  #error = length(subset(results, results=="0.5"))/length(output$newx1)
  
  ###Stackoverflow version -which actually works.
  
  ##generate a new dataset
  xtest = matrix(rnorm(100 * 10), ncol = 10)
  ##randomly assign a 1 or a 0
  ytest = sample(c(1, 0), 100, rep = TRUE)
  xtest[ytest == 1, 1:5] = xtest[ytest == 1, 1:5] + 1
  testdat = data.frame(x = xtest, y = as.factor(ytest))

  #check if the fit above gets it right
  ypred = predict(svm.fit, testdat)
  #make a table of the predictions
  result = table(predict = ypred, truth = testdat$y)
  #calculate the error
  test.err[mtry] = 1 - (result[1] + result[4]) / 100
  #test.err[mtry] = error
  
}

mean(test.err)

```

let me try this other code I found online Stackoverflow.

```{r}
set.seed(1001)
counts = 1000
errate = rep(0, counts)
for(i in 1:counts){
  x = matrix(rnorm(100 * 10), ncol = 10)
  y = c(rep(0, 50), rep(1, 50))
  x[y == 1, 1:5] = x[y == 1, 1:5] + 1 #moves the mean of these values from 0 to 1.
  dat = data.frame(x = x, y = as.factor(y))
  #svm.fit = svm(y ~ ., data = dat, kernel = "linear", cost = 1) #linear
  svm.fit = svm(y ~ ., data = dat) #default radial

  xtest = matrix(rnorm(100 * 10), ncol = 10)
  ytest = sample(c(0, 1), 100, rep = TRUE)
  xtest[ytest == 1, 1:5] = xtest[ytest == 1, 1:5] + 1
  testdat = data.frame(x = xtest, y = as.factor(ytest))

  ypred = predict(svm.fit, testdat)
  result = table(predict = ypred, truth = testdat$y)
  errate[i] = 1 - (result[1] + result[4]) / 100
}
mean(errate)
```


What is the expected test error for logistic regression? (to within 10%)

(Don't worry if you get errors saying the logistic regression did not converge.)

I'm going to have to come back for this.

It all goes a bit pear-shaped here, so don't worry about the code from here down, until I fix it that is.


```{r}
require(ISLR)
set.seed(235)
counts= 1000
test.err = double(counts)
for (mtry in 1:counts){
  x=matrix(rnorm(1000),ncol=10)
  y=rep(c(1,0),c(50,50))
  x[y==1,1:5]=x[y==1,1:5]+1
  
  #dat=data.frame(y=factor(y),x)  #my version
  #fit=svm(factor(y)~.,data=dat)  #my version
  
  #online version - stackoverflow
  dat = data.frame(x = x, y = as.factor(y))
  glm.fit=glm(y~.,data=dat,family=binomial)
  
  
  #svm.fit = svm(y ~ ., data = dat, kernel = "linear", cost = 1) #linear #0.15791
  #svm.fit = svm(y ~ ., data = dat) #default radial ##this version answer is meant to be 0.16350

  ##the above does seem to work-ish, I get 0.16922 in both cases
  
  
  ##my estimate of the percentage
  #out_y = as.numeric(predict(fit,x)) - 1
  #results = (out_y + y)/2.
  #error = length(subset(results, results=="0.5"))/length(output$newx1)
  
  ###Stackoverflow version -which actually works.
  
  ##generate a new dataset
  xtest = matrix(rnorm(100 * 10), ncol = 10)
  ##randomly assign a 1 or a 0
  ytest = sample(c(1, 0), 100, rep = TRUE)
  xtest[ytest == 1, 1:5] = xtest[ytest == 1, 1:5] + 1
  testdat = data.frame(x = xtest, y = as.factor(ytest))

  #check if the fit above gets it right
  ypred = predict(svm.fit, testdat)
  #make a table of the predictions
  result = table(predict = ypred, truth = testdat$y)
  #calculate the error
  test.err[mtry] = 1 - (result[1] + result[4]) / 100
  #test.err[mtry] = error
  
}

mean(test.err)

```